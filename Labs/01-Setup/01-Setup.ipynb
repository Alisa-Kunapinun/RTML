{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satisfactory-command",
   "metadata": {},
   "source": [
    "# RTML Lab 01: Setup\n",
    "\n",
    "In this lab, we'll set up the Python and CUDA environment that we'll use for the rest of the semester.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The recommended environment for your work is Ubuntu Linux 20.04 LTS.\n",
    "\n",
    "You will most likely have the best experience with Ubuntu by installing it natively on your development workstation.\n",
    "\n",
    "However, an alternative path is to run Ubuntu side-by-side with Microsoft Windows using the Windows Subsystem for Linux (WSL).\n",
    "\n",
    "According to our surveys, most DS&AI students are not yet ready to switch to Ubuntu for 100% of their work and would like a more gentle introduction\n",
    "to the use of Linux for software development. Therefore, in this tutorial, we assume you are running Windows.\n",
    "\n",
    "### WSL\n",
    "\n",
    "The first step is to install WSL according to [Microsoft's WSL installation instructions](https://docs.microsoft.com/en-us/windows/wsl/install-win10).\n",
    "\n",
    "When it's time to download a Linux distribution from the Windows store, choose \"Ubuntu 20.04.\"\n",
    "\n",
    "### OpenSSH feature in Windows\n",
    "\n",
    "If OpenSSH is not already enabled, go to the \"Manage Optional Features\" settings panel in Windows, select \"OpenSSH client\", and click \"Install\".\n",
    "\n",
    "More detail is available at [Microsoft's documentation page for OpenSSH installation](https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse#installing-openssh-from-the-settings-ui-on-windows-server-2019-or-windows-10-1809).\n",
    "\n",
    "### VSCode\n",
    "\n",
    "Visual Studio Code is a lightweight yet full featured cross platform IDE for software development that has recently caught up\n",
    "in terms of capabilities and popularity with other popular IDEs for Python such as PyCharm. It is reputed to be easier to configure and use, also.\n",
    "We'll give it a try this semester. Download and install VSCode from [the Visual Studio downloads page](https://code.visualstudio.com/download).\n",
    "\n",
    "## WSL Python environment\n",
    "\n",
    "As a starting point, let's set up Python under WSL.\n",
    "\n",
    "In the lab manuals for RTML, when you see commands beginning with '\\$' like this: \n",
    "\n",
    "    $ ls\n",
    "    \n",
    "it means that you should run the command `ls` at your local WSL prompt.\n",
    "\n",
    "As a first step, you'll need to update Ubuntu's package lists, upgrade any that need upgrading, and then install Python 3 and the PIP package manager:\n",
    "\n",
    "    $ sudo apt-get update\n",
    "    $ sudo apt-get upgrade\n",
    "    $ sudo apt-get install python3-pip\n",
    "\n",
    "Then you should be able to run Python code:\n",
    "\n",
    "    $ python3\n",
    "    ...\n",
    "    >>> print('Hello, world!')\n",
    "    Hello, world!\n",
    "    >>>\n",
    "    \n",
    "(Here the `>>>` prompt means the Python interpreter prompt.)\n",
    "\n",
    "To install Python packages, use PIP:\n",
    "\n",
    "    $ pip3 install torch\n",
    "    ...\n",
    "    $ python3\n",
    "    ...\n",
    "    >>> import torch\n",
    "    >>> torch.__version__\n",
    "    '1.7.1'\n",
    "   \n",
    "That was easy, right?\n",
    "\n",
    "To work at a professional level with Python, you'll probably want to look at `virtualenv`, which allows you to\n",
    "maintain separate Python environments with all dependencies for each of your projects separately.\n",
    "If you like, take a look at [the Python docs on virtualenv](https://docs.python-guide.org/dev/virtualenvs/#lower-level-virtualenv) for more information.\n",
    "Note that in Ubuntu, the commands are `python3` and `pip3`, not `python` and `pip`.\n",
    "\n",
    "In any case, in RTML, we will do most of our work remotely with another technology, Docker, to isolate our projects, so we don't need to get into virtualenv right now.\n",
    "\n",
    "## SSH setup for remote access to GPU server in WSL\n",
    "\n",
    "Next, we will set up our environment for access to a remote GPU server via SSH (the Secure Shell protocol).\n",
    "\n",
    "We'll begin with WSL then use the same configuration for our Windows SSH configuration.\n",
    "\n",
    "We'll assume that your GPU server is behind a gateway.\n",
    "\n",
    "At AIT, the gateway is `bazooka.cs.ait.ac.th`, and the GPU server is `puffer.cs.ait.ac.th`. I'll assume your username on both the\n",
    "gateway and GPU server is `st123456`. If you are using a different gateway or server, replace these names with your\n",
    "specific ones in the rest of the tutorial, and obviously, replace `st123456` with your username.\n",
    "\n",
    "First, let's try connecting to the gateway:\n",
    "\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    Password for st123456@bazooka.cs.ait.ac.th:\n",
    "    ...\n",
    "    st123456@bazooka:~$ [Control-D or \"exit\" to exit]\n",
    "\n",
    "Next, we want to avoid having to type a password every time we log in to the remote server.\n",
    "We will generate an RSA public/private keypair for SSH to allow login without a password.\n",
    "If you already have an RSA public/private keypair for SSH, you can skip this step.\n",
    "\n",
    "    $ ssh-keygen -t rsa\n",
    "    Generating public/private rsa key pair.\n",
    "    Enter file in which to save the key (/home/mdailey/.ssh/id_rsa) [ENTER]\n",
    "    Enter passphrase (empty for no passphrase): [USE A PASSPHRASE YOU'LL NEVER FORGET]\n",
    "    Enter same passphrase again:\n",
    "    Your identification has been saved in /home/mdailey/.ssh/id_rsa\n",
    "    Your public key has been saved in /home/mdailey/.ssh/id_rsa.pub\n",
    "    The key fingerprint is:\n",
    "    SHA256:AgTtgfplWmns7Z0bQBOuYOawrKff0zZiI4rOVOVLHww mdailey@LAPTOP-NE58KA3C\n",
    "    The key's randomart image is:\n",
    "    +---[RSA 3072]----+\n",
    "    |  .+. .          |\n",
    "    |  ..o. .         |\n",
    "    |..+o.E+          |\n",
    "    |o* .@+o.         |\n",
    "    |.o.O.+ooS        |\n",
    "    |. + o +o.        |\n",
    "    |...  + o..       |\n",
    "    |+o..= = o.       |\n",
    "    |==.o.= ...       |\n",
    "    +----[SHA256]-----+\n",
    "    $\n",
    "\n",
    "Next, we copy the PUBLIC key to the server and tell the server to accept our login using the corresponding private key:\n",
    "\n",
    "    $ scp .ssh/id_rsa.pub st123456@bazooka.cs.ait.ac.th:\n",
    "    Password for st123456@bazooka.cs.ait.ac.th:\n",
    "    ...\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    Password for st123456@bazooka.cs.ait.ac.th:\n",
    "    ...\n",
    "    bazooka$ mkdir -p .ssh\n",
    "    bazooka$ cat id_rsa.pub >> .ssh/authorized_keys\n",
    "    bazooka$ exit\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    Enter passphrase for key '/home/mdailey/.ssh/id_rsa': [USE THAT PASSPHRASE YOU'LL NEVER FORGET]\n",
    "    ...\n",
    "    bazooka$ exit\n",
    "    $ \n",
    "\n",
    "We don't have to enter our password for the remote server anymore, but we still have to enter our passphrase for the key file. To fix that, we need something called the SSH Agent!\n",
    "It's a little program that runs in the background, reads your private keys into memory (if you ask it to), and then later supplies your keys to SSH every time authentication is needed.\n",
    "\n",
    "    $ eval `ssh-agent`\n",
    "    $ ssh-add ~/.ssh/id_rsa\n",
    "    Enter passphrase for /home/mdailey/.ssh/id_rsa:\n",
    "    Identity added: /home/mdailey/.ssh/id_rsa (mdailey@LAPTOP-NE58KA3C)\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    ...\n",
    "    bazooka$ exit\n",
    "\n",
    "OK! Now that we can jump to the gateway without a password, let's use it to jump to the GPU server. For this, you need a file `~/.ssh/config` with contents\n",
    "\n",
    "    Host puffer\n",
    "      Hostname puffer.cs.ait.ac.th\n",
    "      ProxyCommand ssh bazooka.cs.ait.ac.th -W %h:%p\n",
    "      User st123456\n",
    "      ForwardAgent yes\n",
    "  \n",
    "If everything is OK, you should now be able to SSH directly to the GPU server without using passwords or passphrases:\n",
    "\n",
    "    $ ssh puffer\n",
    "    puffer$ exit\n",
    "\n",
    "That was a lot of steps, but not too bad, right?\n",
    "\n",
    "## SSH setup for remote access to GPU server in Windows\n",
    "\n",
    "Now that everything is set up in WSL, it's easy to get it working in Windows directly. VSCode will use Windows' SSH client to connect to our GPU server, so that's why we need to set up\n",
    "both.\n",
    "\n",
    "Within WSL, copy your RSA key files and remote host configuration to your Windows home directory:\n",
    "\n",
    "    $ mkdir -p /mnt/c/Users/Matthew\\ Dailey/.ssh\n",
    "    $ cp ~/.ssh/config /mnt/c/Users/Matthew\\ Dailey/.ssh/\n",
    "    $ cp ~/.ssh/id_rsa* /mnt/c/Users/Matthew\\ Dailey/.ssh/\n",
    "\n",
    "In the Windows Powershell RUNNING AS ADMINISTRATOR, tell Windows to always start the SSH Agent:\n",
    "\n",
    "    PS C:\\WINDOWS/system32> Set-Service ssh-agent -StartupType Automatic\n",
    "    PS C:\\WINDOWS/system32> Start-Service ssh-agent\n",
    "    PS C:\\WINDOWS/system32> Get-Service ssh-agent\n",
    "    \n",
    "    Status   Name               DisplayName\n",
    "    ------   ----               -----------\n",
    "    Running  ssh-agent          OpenSSH Authentication Agent\n",
    "\n",
    "    PS C:\\WINDOWS/system32> exit\n",
    "\n",
    "Now, in an ordinary Powershell, you should be able to log in to the GPU server:\n",
    "\n",
    "    PS C:\\Users\\Matthew Dailey> ssh puffer\n",
    "    ...\n",
    "    puffer$\n",
    "\n",
    "\n",
    "## Testing NVIDIA/Docker integration\n",
    "\n",
    "If you have docker version 19.03 or later installed on the server, you don't need to use a special program to access NVIDIA GPUs within docker. Try the following:\n",
    "\n",
    "    puffer$ docker run -it --rm --gpus all ubuntu nvidia-smi\n",
    "\n",
    "Once the docker images download, you should see some information about the GPU environment on the system. Try the following for a detailed list of GPUs you have access to:\n",
    "\n",
    "    puffer$ docker run -it --rm --gpus all ubuntu nvidia-smi -L\n",
    "    Fri Jan 15 00:58:18 2021\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: N/A      |\n",
    "    |-------------------------------+----------------------+----------------------+\n",
    "    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "    |                               |                      |               MIG M. |\n",
    "    |===============================+======================+======================|\n",
    "    |   0  GeForce RTX 208...  Off  | 00000000:84:00.0 Off |                  N/A |\n",
    "    | 24%   38C    P0    56W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    |   1  GeForce RTX 208...  Off  | 00000000:85:00.0 Off |                  N/A |\n",
    "    | 22%   40C    P0    47W / 250W |      0MiB / 11019MiB |      1%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    |   2  GeForce RTX 208...  Off  | 00000000:88:00.0 Off |                  N/A |\n",
    "    | 23%   36C    P0    51W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    |   3  GeForce RTX 208...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
    "    | 31%   36C    P0    25W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    \n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | Processes:                                                                  |\n",
    "    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "    |        ID   ID                                                   Usage      |\n",
    "    |=============================================================================|\n",
    "    |  No running processes found                                                 |\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    puffer$\n",
    "\n",
    "One fun trick is to use the DIGITS web application for a deep learning experiment:\n",
    "\n",
    "    puffer$ docker run -itd --gpus all -p 5000:5000 nvidia/digits\n",
    "\n",
    "You'll have to pick an unused port. Once running, check that the server is actually up:\n",
    "\n",
    "    puffer$ wget http://localhost:5000\n",
    "\n",
    "If successful, you're up. To forward local port 5000 to port 5000 on the GPU server, you need to modify the command you use to ssh to the server:\n",
    "\n",
    "    $ ssh -L 5000:localhost:5000 puffer\n",
    "    puffer$\n",
    "\n",
    "Now you should be able to access http://localhost:5000 in your Web browser and access the DIGITS application running on the server.\n",
    "\n",
    "## Build your docker image\n",
    "\n",
    "To build a Docker image, you'll need to access the Internet. Unfortunately, on the GPU server,\n",
    "we require a proxy server to access the Internet. To set a proxy with CSH, edit ~/.cshrc and put this at the end of the file:\n",
    "\n",
    "setenv http_proxy http://192.41.170.23:3128\n",
    "setenv https_proxy http://192.41.170.23:3128\n",
    "\n",
    "For BASH, you would edit ~/.bashrc and place the following at the end of the file:\n",
    "\n",
    "export http_proxy=http://192.41.170.23:3128\n",
    "export https_proxy=http://192.41.170.23:3128\n",
    "\n",
    "Now, let's prepare the docker environment on the GPU server\n",
    "\n",
    "    $ scp ~/.ssh/id_rsa.pub puffer:\n",
    "    $ ssh puffer\n",
    "    puffer$ mkdir -p lab\n",
    "    puffer$ mv id_rsa.pub lab/\n",
    "    puffer$ cd lab\n",
    "    puffer$ cat > Dockerfile <<EOF\n",
    "    FROM nvidia/cuda:10.1-base\n",
    "\n",
    "    ENV http_proxy http://192.41.170.23:3128\n",
    "    ENV https_proxy http://192.41.170.23:3128\n",
    "\n",
    "    RUN apt-get update && apt-get upgrade -y && apt-get install -y openssh-server\n",
    "    RUN mkdir /var/run/sshd\n",
    "    RUN mkdir /root/.ssh/\n",
    "    COPY id_rsa.pub /root/.ssh/authorized_keys\n",
    "    EXPOSE 22\n",
    "\n",
    "    # set the locale to en_US.UTF-8\n",
    "    RUN apt-get install -y locales\n",
    "    ENV DEBIAN_FRONTEND noninteractive\n",
    "    RUN echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen \\\n",
    "        && locale-gen en_US.UTF-8 \\\n",
    "        && dpkg-reconfigure locales \\\n",
    "        && /usr/sbin/update-locale LANG=en_US.UTF-8\n",
    "    ENV LC_ALL en_US.UTF-8\n",
    "\n",
    "    RUN apt-get install -y python3-pip\n",
    "    RUN pip3 install numpy torch torchvision\n",
    "\n",
    "    CMD [\"/usr/sbin/sshd\", \"-D\"]\n",
    "    EOF\n",
    "    puffer$ docker build . -t matt-lab1   # Use your own tag here!\n",
    "\n",
    "If the docker image builds successfully, you're ready to go! Run the image as\n",
    "\n",
    "    puffer$ docker run -p 2222:22 --gpus all matt-lab1\n",
    "\n",
    "You'll want to use a unique port name in place of 2222 if there are others using the GPU server. If that works, you should be able to run\n",
    "\n",
    "    puffer$ ssh root@localhost -p 2222\n",
    "\n",
    "(Use whatever port you selected above in place of 2222, of course.) If that worked, now you can ssh to the gpu server and open a local port to the remote system:\n",
    "\n",
    "    $ ssh -L 2222:localhost:2222 puffer \"docker run -p 2222:22 --gpus all matt-lab1\"\n",
    "\n",
    "The first 2222 is the port on the LOCAL machine. There won't be any conflict there, so you can use what you like. The second port is the port on the REMOTE machine. It should match the port used on the GPU machine for the docker image's SSH port (also 2222 in the example above).\n",
    "\n",
    "If all that worked, now your Docker container is up and running your image with GPU support, and local port 2222 is forwarded to the SSH port of the new Docker container. Now you should be able to run (on your local system)\n",
    "\n",
    "    $ ssh root@localhost -p 2222\n",
    "    1f7a17f71d25# pip3 list\n",
    "\n",
    "You should see torch and numpy among the list of installed Python packages. All is good!\n",
    "\n",
    "## Connect VSCode to your container on the GPU server\n",
    "\n",
    "Now we are almost done. The last step is to tell VSCode to use the remote environment inside the new Docker container rather than the local system for running our Python code.\n",
    "\n",
    "Use the Remote SSH extension in VSCode to connect with your container on the server.\n",
    "\n",
    "To test that all is well, create a Python script and enter the code\n",
    "\n",
    "    import torch\n",
    "    torch.has_cuda\n",
    "\n",
    "If you get the result True, you're successful!\n",
    "\n",
    "## Try out an AlexNet model\n",
    "\n",
    "Try some sample code executing the AlexNet model in PyTorch:\n",
    "\n",
    "    import torch\n",
    "    import urllib\n",
    "    import os\n",
    "\n",
    "    os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "    os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "    model = torch.hub.load('pytorch/vision:v0.5.0', 'alexnet', pretrained=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Download an example image from the pytorch website\n",
    "\n",
    "    url, filename = (\"https://github.com/pytorch/hub/raw/master/dog.jpg\", \"dog.jpg\")\n",
    "    try:\n",
    "        urllib.URLopener().retrieve(url, filename)\n",
    "    except:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "    input_image = Image.open(filename)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "    #print(output[0])\n",
    "\n",
    "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "    softmax_scores = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    maxval, maxindex = output.max(1)\n",
    "    print('Maximum value', maxval, 'at index', maxindex)\n",
    "\n",
    "I got an evaluation time of 24 ms for CPU and 2 ms for CUDA on puffer.\n",
    "\n",
    "## The independent part\n",
    "\n",
    "OK, so now your goal is to try to train and evaluate the PyTorch AlexNet model on the CIFAR-10 dataset. You can use Torch's torchvision module to load the data into PyTorch tensors.\n",
    "\n",
    "## The report\n",
    "\n",
    "Your lab report should have the following sections:\n",
    "\n",
    "1. Introduction: the background and goals of the lab\n",
    "1. Methods: what you did, what parameters you tried, and so on\n",
    "1. Results: what were the results\n",
    "1. Conclusion: what did you learn from the lab, and what might be the next steps\n",
    "\n",
    "In the results section, be sure to show training and validation loss as a function of training epochs. You'll also want to show results on a separate test set and give some analysis of the errors the classifier makes on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-punch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
