{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab 10: RNNs\n",
    "\n",
    "Today we'll do some preliminary investigation of simple RNNs similar to the form of Figure 10.3 in Goodfellow et al."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## First tutorial: Predicting the language a surname comes from\n",
    "\n",
    "Let's begin with one of the official PyTorch tutorials on classifying surnames from 18 languages based on the character sequence. (https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    "\n",
    "## Runtime environment\n",
    "\n",
    "For the runtime environment, we won't get much benefit from the GPU in today's lab, as the examples are not batched, and it's not easy to do so, since different words have different lengths. Since the GPU server isn't going to help us much, we'll run locally using Jupyter.\n",
    "\n",
    "## Surnames dataset\n",
    "Get your Jupyter environment up and running, then download the dataset (https://download.pytorch.org/tutorial/data.zip) and unzip it in your project directory.\n",
    "\n",
    "Here's code directly from the tutorial to read the names into a dictionary of the form { language1: [name1, name2, ...], language2: ... }"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['data/names/Spanish.txt', 'data/names/Irish.txt', 'data/names/Chinese.txt', 'data/names/Arabic.txt', 'data/names/Vietnamese.txt', 'data/names/English.txt', 'data/names/Russian.txt', 'data/names/Portuguese.txt', 'data/names/Dutch.txt', 'data/names/Scottish.txt', 'data/names/Greek.txt', 'data/names/Czech.txt', 'data/names/Italian.txt', 'data/names/Korean.txt', 'data/names/French.txt', 'data/names/Polish.txt', 'data/names/Japanese.txt', 'data/names/German.txt']\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spanish\n['Abana', 'Abano', 'Abarca', 'Abaroa', 'Abascal', 'Abasolo', 'Abel', 'Abello', 'Aberquero', 'Abreu', 'Acosta', 'Agramunt', 'Aiza', 'Alamilla', 'Albert', 'Albuquerque', 'Aldana', 'Alfaro', 'Alvarado', 'Alvarez', 'Alves', 'Amador', 'Andreu', 'Antunez', 'Aqua', 'Aquino', 'Araujo', 'Araullo', 'Araya', 'Arce', 'Arechavaleta', 'Arena', 'Aritza', 'Armando', 'Arreola', 'Arriola', 'Asis', 'Asturias', 'Avana', 'Azarola', 'Banderas', 'Barros', 'Basurto', 'Bautista', 'Bello', 'Belmonte', 'Bengochea', 'Benitez', 'Bermudez', 'Blanco', 'Blanxart', 'Bolivar', 'Bonaventura', 'Bosque', 'Bustillo', 'Busto', 'Bustos', 'Cabello', 'Cabrera', 'Campo', 'Campos', 'Capello', 'Cardona', 'Caro', 'Casales', 'Castell', 'Castellano', 'Castillion', 'Castillo', 'Castro', 'Chavarria', 'Chavez', 'Colon', 'Costa', 'Crespo', 'Cruz', 'Cuellar', 'Cuevas', \"D'cruz\", \"D'cruze\", 'De la cruz', 'De la fuente', 'Del bosque', 'De leon', 'Delgado', 'Del olmo', 'De santigo', 'Diaz', 'Dominguez', 'Duarte', 'Durante', 'Echevarria', 'Echeverria', 'Elizondo', 'Escamilla', 'Escarcega', 'Escarra', 'Esparza', 'Espina', 'Espino', 'Espinosa', 'Espinoza', 'Estevez', 'Etxebarria', 'Etxeberria', 'Felix', 'Fernandez', 'Ferrer', 'Fierro', 'Flores', 'Fonseca', 'Franco', 'Fuentes', 'Gallego', 'Gallo', 'Garcia', 'Garrastazu', 'Garza', 'Gaspar', 'Gebara', 'Gomez', 'Gonzales', 'Gonzalez', 'Grec', 'Guadarrama', 'Guerra', 'Guerrero', 'Gutierrez', 'Gutierrez', 'Hernandez', 'Herrera', 'Herrero', 'Hierro', 'Holguin', 'Huerta', 'Ibanez', 'Ibarra', 'Iniguez', 'Iturburua', 'Jaso', 'Jasso', 'Jimenez', 'Jorda', 'Juarez', 'Lobo', 'Lopez', 'Losa', 'Loyola', 'Machado', 'Macias', 'Maradona', 'Maria', 'Marino', 'Marquez', 'Martell', 'Marti', 'Martinez', 'Martinez', 'Mas', 'Mata', 'Mateu', 'Medina', 'Melendez', 'Mendez', 'Mendoza', 'Menendez', 'Merlo', 'Michel', 'Mingo', 'Moles', 'Molina', 'Montero', 'Morales', 'Moralez', 'Moreno', 'Narvaez', 'Nieves', 'Noguerra', 'Nunez', 'Obando', 'Ochoa', 'Ojeda', 'Ola', 'Oleastro', 'Olguin', 'Oliver', 'Olmos', 'Oquendo', 'Orellana', 'Oriol', 'Ortega', 'Ortiz', 'Palomo', 'Paredes', 'Pavia', 'Pelaez', 'Pena', 'Perez', 'Perez', 'Petit', 'Picasso', 'Porra', 'Porras', 'Prieto', 'Puerta', 'Puga', 'Puig', 'Quinones', 'Quintana', 'Quiros', 'Ramirez', 'Ramos', 'Rana', 'Rendon', 'Rey', 'Reyes', 'Rios', 'Rivera', 'Rivero', 'Robledo', 'Robles', 'Rocha', 'Rodriguez', 'Rodriquez', 'Roig', 'Rojas', 'Rojo', 'Roldan', 'Roma', 'Roma', 'Romero', 'Rosa', 'Rosales', 'Rubio', 'Ruiz', 'Sala', 'Salamanca', 'Salazar', 'Salcedo', 'Salinas', 'Sanchez', 'Sandoval', 'San nicolas', 'Santana', 'Santiago', 'Santillian', 'Santos', 'Sastre', 'Sepulveda', 'Sierra', 'Silva', 'Soler', 'Solo', 'Solos', 'Soto', 'Suarez', 'Suero', 'Tapia', 'Terrazas', 'Tomas', 'Torres', 'Tos', 'Tosell', 'Toset', 'Travieso', 'Trujillo', 'Ubina', 'Urbina', 'Urena', 'Valdez', 'Valencia', 'Varela', 'Vargas', 'Vasquez', 'Vazquez', 'Vega', 'Vela', 'Vela', 'Velazquez', 'Ventura', 'Vicario', 'Vilaro', 'Villa', 'Villalobos', 'Villanueva', 'Villaverde', 'Viola', 'Viteri', 'Vivas', 'Vives', 'Ybarra', 'Zabala', 'Zambrano', 'Zamorano', 'Zapatero', 'Zavala', 'Zubizarreta', 'Zuniga']\nIrish\n['Adam', 'Ahearn', 'Aodh', 'Aodha', 'Aonghuis', 'Aonghus', 'Bhrighde', 'Bradach', 'Bradan', 'Braden', 'Brady', 'Bran', 'Brannon', 'Brian', 'Callaghan', 'Caomh', 'Carey', 'Casey', 'Cassidy', 'Cathain', 'Cathan', 'Cathasach', 'Ceallach', 'Ceallachan', 'Cearbhall', 'Cennetig', 'Ciardha', 'Clark', 'Cleirich', 'Cleirigh', 'Cnaimhin', 'Coghlan', 'Coilean', 'Collins', 'Colman', 'Conall', 'Conchobhar', 'Conn', 'Connell', 'Connolly', 'Cormac', 'Corraidhin', 'Cuidightheach', 'Curran', 'Dubhshlaine', 'Dalach', 'Daly', 'Damhain', 'Damhan', 'Delaney', 'Desmond', 'Devin', 'Diarmaid', 'Doherty', 'Domhnall', 'Donnchadh', 'Donndubhan', 'Donnell', 'Donoghue', 'Donovan', 'Doyle', 'Dubhain', 'Dubhan', 'Duncan', 'Eoghan', 'Eoin', 'Eoin', 'Faolan', 'Farrell', 'Fearghal', 'Fergus', 'Finn', 'Finnegan', 'Fionn', 'Flanagan', 'Flann', 'Flynn', 'Gallchobhar', 'Gerald', 'Giolla', 'Gorman', 'Hayden', 'Ivor', 'John', 'Kavanagh', 'Keefe', 'Kelly', 'Kennedy', 'Lennon', 'Login', 'Macclelland', 'Macdermott', 'Maceachthighearna', 'Macfarland', 'Macghabhann', 'Maciomhair', 'Macshuibhne', 'Madaidhin', 'Madden', 'Maguire', 'Mahoney', 'Maille', 'Malone', 'Manus', 'Maolmhuaidh', 'Mathghamhain', 'Maurice', 'Mcguire', 'Mckay', 'Mclain', 'Mcmahon', 'Mcnab', 'Mcneil', 'Meadhra', 'Michael', 'Milligan', 'Mochan', 'Mohan', 'Molloy', 'Monahan', 'Mooney', 'Muirchertach', 'Mullen', 'Mulryan', 'Murchadh', 'Murphy', 'Names', 'Naoimhin', 'Naomhan', 'Neil', 'Neville', 'Nevin', 'Niadh', 'Niall', 'Nolan', 'Nuallan', \"O'Boyle\", \"O'Brien\", \"O'Byrne\", \"O'Donnell\", \"O'Hannagain\", \"O'Hannigain\", \"O'Keefe\", \"O'Mooney\", \"O'Neal\", \"O'Boyle\", \"O'Bree\", \"O'Brian\", \"O'Brien\", \"O'Callaghann\", \"O'Connell\", \"O'Connor\", \"O'Dell\", \"O'Doherty\", \"O'Donnell\", \"O'Donoghue\", \"O'Dowd\", \"O'Driscoll\", \"O'Gorman\", \"O'Grady\", \"O'Hagan\", \"O'Halloran\", \"O'Hanlon\", \"O'Hara\", \"O'Hare\", \"O'Kane\", \"O'Keefe\", \"O'Keeffe\", \"O'Kelly\", \"O'Leary\", \"O'Loughlin\", \"O'Mahoney\", \"O'Mahony\", \"O'Malley\", \"O'Meara\", \"O'Neal\", \"O'Neill\", \"O'Reilly\", \"O'Rourke\", \"O'Ryan\", \"O'Shea\", \"O'Sullivan\", \"O'Toole\", 'Patrick', 'Peatain', 'Pharlain', 'Power', 'Quigley', 'Quinn', 'Quirke', 'Raghailligh', 'Reagan', 'Register', 'Reilly', 'Reynold', 'Rhys', 'Riagain', 'Riagan', 'Riain', 'Rian', 'Rinn', 'Roach', 'Rodagh', 'Rory', 'Ruadh', 'Ruadhain', 'Ruadhan', 'Ruaidh', 'Samuel', 'Scolaidhe', 'Seaghdha', 'Sechnall', 'Seighin', 'Shannon', 'Sheehy', 'Simon', 'Sioda', 'Sloan', 'Sluaghadhan', 'Suaird', 'Sullivan', 'Tadhg', 'Tadhgan', 'Taidhg', 'Teagan', 'Teague', 'Tighearnach', 'Tracey', 'Treasach', 'Whalen', 'Whelan', 'William']\n"
     ]
    }
   ],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "# test\n",
    "for c in all_categories[:2]:\n",
    "    print(c)\n",
    "    print(category_lines[c]) "
   ]
  },
  {
   "source": [
    "OK, try it out. You can see some results with a query like"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "source": [
    "## One-hot Encoding (What is it?)\n",
    "\n",
    "reference: https://victorzhou.com/blog/one-hot/\n",
    "\n",
    "One-Hot Encoding is a single integer and produces a vector where a single element is 1 and all other elements are 0, like [0, 1, 0, 0]. The encoding can be used in text generator because we can assume that all characters are orthogonal together. This is known as integer encoding. For Machine Learning, this encoding can be problematic.\n",
    "\n",
    "|Character|Value|One-Hot|\n",
    "|---------|-----|-------|\n",
    "|A|1|1 0 0 0 ...|\n",
    "|B|2|0 1 0 0 ...|\n",
    "|C|3|0 0 1 0 ...|\n",
    "\n",
    "Not only characters can be generated as one-hot, but also words, such as color, tree, and so on.\n",
    "\n",
    "|Word|Value|One-Hot|\n",
    "|---------|-----|-------|\n",
    "|Red|1|1 0 0 0 ...|\n",
    "|Blue|2|0 1 0 0 ...|\n",
    "|Green|3|0 0 1 0 ...|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## One-hot representation of characters\n",
    "Next, we'll convert each letter in each word to a one-hot representation, for example \"b\" = < 0 1 0 0 0 ...>. The tensor size will be <linelength x 1 x nletters>. The first dimension is the number of characters in a given line of a data file, the second dimension is the index into the batch (we have a batch size of 1 here), and the third dimension indexes the different characters.\n",
    "\n",
    "Here's how to do it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example in other library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Using scikit-learn’s OneHotEncoder:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "print(encoder.fit_transform([['red'], ['green'], ['blue']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Using numpy\n",
    "import numpy as np\n",
    "\n",
    "arr = [2, 1, 0]\n",
    "max = np.max(arr) + 1\n",
    "print(np.eye(max)[arr])"
   ]
  },
  {
   "source": [
    "### Turn back to real code: using pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  }
 ]
}