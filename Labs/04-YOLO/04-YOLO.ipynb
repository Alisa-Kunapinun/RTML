{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Lab 04: YOLO\n",
    "\n",
    "In this lab, we'll explore a fascinating use of image classification deep neural networks to peform a different\n",
    "task: object detection.\n",
    "\n",
    "## Object Detection\n",
    "\n",
    "If you could go back in time to the 1990s, there were no cameras that could find faces in a photograph, and no\n",
    "researcher had a way to count dogs in a video in real time. Everyone had to count the dogs manually.\n",
    "Times were very tough.\n",
    "\n",
    "The Holy Grail of computer vision research at the time was real time face detection. If we could find faces\n",
    "in images fast enough, we could build systems that interact more naturally with human beings. But nobody had\n",
    "a solution.\n",
    "\n",
    "Things changed when Viola and Jones introduced the first real time face detector, the Haar-like cascade, at\n",
    "the end of the 1990s.\n",
    "This technique swept a detection window over the input image at multiple sizes, and subjected each local patch\n",
    "to a cascade of simple rough classifiers. Each patch that made it to the end of the cascade of classifiers was\n",
    "treated as a positive detection. After a set of candidate patches were identified, there would be a cleanup\n",
    "stage when neighboring detections are clustered into isolated detections.\n",
    "\n",
    "This method and one cousin, the HOG detector, which was slower but a little more accurate, dominated during the 2000s\n",
    "and on into the 2010s. These methods worked well enough when trained carefully on the specific environment they were\n",
    "used in, but usually couldn't be transfer to a new environment.\n",
    "\n",
    "With the introduction of AlexNet and the amazing advances in image classification, we could follow the direction\n",
    "of R-CNN, to use a region proposal algorithm followed by a deep learning classifier to do object detection VERY slowly\n",
    "but much more accurately than the old real time methods.\n",
    "\n",
    "## What is YOLO?\n",
    "\n",
    "However, it wasn't until YOLO that we had a deep learning model for object detection that could run in real time.\n",
    "It took some clever insight to realize that everything, from feature extraction to bounding box estimation, could\n",
    "actually be done in a single model that could be trained end-to-end to detect objects.\n",
    "\n",
    "YOLO (You Only Look Once) uses only convolutional layers. This makes it a \"fully convolutional network\" or FCN.\n",
    "\n",
    "YOLOv3 has 75 convolutional layers, with skip connections and upsampling layers. No pooling is used, but there is a convolutional\n",
    "layer with stride 2 used for downsampling to prevent loss of low-level features when use pooling.\n",
    "\n",
    "Normally, the output of a convolutional layer is a feature map, which is then used for detection prediction.\n",
    "However, the innovation of YOLO was to uses the feature map directly to predict bounding boxes and, for each bounding box, to\n",
    "predict whether or not an object is at the center of the bounding box. Finally, a classifier is used for each bounding box\n",
    "to indicate the content of the bounding box.\n",
    "\n",
    "## YOLO v3 from \"scratch\"\n",
    "\n",
    "Here we'll experiment with building up the YOLO v3 model in PyTorch. However, we won't train it ourselves; we'll\n",
    "grab the weights from the original Darknet model by Joseph Redmon and friends.\n",
    "\n",
    "This tutorial is based on https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch.\n",
    "\n",
    "A blog by the author: https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/\n",
    "\n",
    "### Ground Truth Bounding Boxes\n",
    "\n",
    "Here is how we present example images and corresponding object bounding boxes to the model.\n",
    "\n",
    "The input image is divided into grid cells. The number of cells depends on the number of convolutional layers\n",
    "and the stride of each of those convolutional layers. For example, if we use a 416$\\times$416 input image size,\n",
    "and we apply 5 conv layers with a stride of 2 each (for a total downsampling factor of 32), we end up with a 13$\\times$13\n",
    "feature map, each corresponding to a region in the original image of size 32$\\times$32 pixels.\n",
    "\n",
    "A ground truth box has a center (x and y position), a width, and a height. Normally the ground truth boxes would be\n",
    "provided by a human annotator.\n",
    "\n",
    "Each ground truth box's center must lie in some grid cell in the original image. Consider this example from the YOLO paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/yolo05.png\" title=\"GroundTruthBox\" style=\"width: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid is represented by the black lines. The ground truth bounding box for the object is the yellow rectangle. The center\n",
    "of this bounding box happens to be within the red-outlined grid cell.\n",
    "\n",
    "The grid cell containing the center of a ground truth bounding box is given the responsibility during training to try to predict\n",
    "the presence of the object.\n",
    "\n",
    "In order to indicate the presence of the given object, the model outputs several parameters for a given candidate object:\n",
    " - $(t_x, t_y, t_w, t_h)$ indicate the box's location and size. During training, the targets for these outputs are the actual ground truth box parameters.\n",
    " - $p_o$ is an \"objectness\" score that indicates the likelihood that an object exists in the given bounding box. This output uses a sigmoid function.\n",
    "   During training, the target for $p_o$ is set to 1 for the center grid cell (the red grid cell), and it is set to 0 for the the neighboring grid cells.\n",
    " - $(p_1, p_2, \\ldots, p_n)$ are class confidence scores. They indicate the probability of the detected object belonging to a particular class. The targets\n",
    "   obviously, are set to 1 for the ground truth object class and 0 for other classes during training.\n",
    "\n",
    "### Anchor Boxes\n",
    "\n",
    "One problem that would occur in YOLO if you tried to directly learn the parameters mentioned above is the problem of unstable gradients during training.\n",
    "In a way that is sort of analagous to how a residual block begins with an identity map and learns differences from identity, YOLO uses the idea of\n",
    "anchor boxes introduced by the R-CNN team. Instead of predicting $(t_x, t_y, t_w, t_h)$ directly, we predict how those parameters are *different from\n",
    "the parameters of a typical bounding box, an anchor box*.\n",
    "YOLO uses three bounding boxes per cell. At training time, once ground truth bounding box's center is mapped to a grid cell, we find which of the anchors for that cell has the highest\n",
    "IoU with the ground truth box.\n",
    "\n",
    "### So What Does YOLO Actually Predict?\n",
    "\n",
    "First, let's understand that all predictions are relative to the grid cell. YOLO predicts the following:\n",
    "- Offsets $(t_x, t_y)$ are specified relative to the top left corner of the grid cell, as a ratio between 0 and 1, using a sigmoid to limit the values.\n",
    "- Height, and width $(t_w, t_h)$ are specified relative to the dimensions of an anchor box.\n",
    "\n",
    "Thus, YOLO does not predict absolute coordinates -- it predicts values that can then be used to compute the box's position and size in absolute coordinates.\n",
    "This diagram gives the idea. We see that the absolute $t_x$ is the grid cell's $(c_x, c_y)$ plus $\\sigma(t_x)$ times the grid cell width. Similarly for $t_y$.\n",
    "The absolute width of the predicted bounding box is the width of the anchor box times $e^{tw}$. Similarly for the height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/yolo06.png\" title=\"GroundTruthBox\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-scale prediction\n",
    "\n",
    "Rather than a single grid size and grid cell size,\n",
    "YOLOv3 detects objects at multiple sizes with downsampling factors of 32, 16, and 8. The largest objects are detected at the\n",
    "first, coarsest scale, whereas mid-sized objects are detected at the intermediate scale, and small objects are detected at the finest\n",
    "scale. The example below shows the three grid sizes relative to the image and an object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/yolo_Scales.png\" title=\"GroundTruthBox\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Building YOLO in PyTorch\n",
    "\n",
    "First of all, we will need OpenCV:\n",
    "\n",
    "    pip3 install --upgrade pip\n",
    "    pip install matplotlib opencv-python\n",
    "\n",
    "Create a directory where the code for detector will live.\n",
    "\n",
    "In that directory, download util.py and darknet.py from https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch.\n",
    "\n",
    "In Jupyter you can download thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-05 00:55:09--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 11533 (11K) [text/plain]\n",
      "Saving to: 'darknet.py'\n",
      "\n",
      "darknet.py          100%[===================>]  11.26K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-02-05 00:55:09 (18.9 MB/s) - 'darknet.py' saved [11533/11533]\n",
      "\n",
      "--2021-02-05 00:55:10--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 7432 (7.3K) [text/plain]\n",
      "Saving to: 'util.py'\n",
      "\n",
      "util.py             100%[===================>]   7.26K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-05 00:55:11 (31.2 MB/s) - 'util.py' saved [7432/7432]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Look at the YOLO Darknet Configuration File\n",
    "\n",
    "Next, let's download the `yolov3.cfg` configuration file and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-05 00:57:12--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/cfg/yolov3.cfg\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 8346 (8.2K) [text/plain]\n",
      "Saving to: 'yolov3.cfg'\n",
      "\n",
      "yolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-05 00:57:13 (16.9 MB/s) - 'yolov3.cfg' saved [8346/8346]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p cfg\n",
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/cfg/yolov3.cfg\n",
    "!mv yolov3.cfg cfg/yolov3.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file looks like this:\n",
    "\n",
    "```python\n",
    "[net]\n",
    "# Testing\n",
    "batch=1\n",
    "subdivisions=1\n",
    "# Training\n",
    "# batch=64\n",
    "# subdivisions=16\n",
    "width= 416\n",
    "\n",
    "height = 416\n",
    "channels=3\n",
    "momentum=0.9\n",
    "decay=0.0005\n",
    "angle=0\n",
    "saturation = 1.5\n",
    "exposure = 1.5\n",
    "hue=.1\n",
    "\n",
    "learning_rate=0.001\n",
    "burn_in=1000\n",
    "max_batches = 500200\n",
    "policy=steps\n",
    "steps=400000,450000\n",
    "scales=.1,.1\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=32\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "...\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "...\n",
    "\n",
    "[yolo]\n",
    "mask = 6,7,8\n",
    "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "classes=80\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\n",
    "\n",
    "[route]\n",
    "layers = -4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[upsample]\n",
    "stride=2\n",
    "\n",
    "[route]\n",
    "layers = -1, 61\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the Configuration Blocks\n",
    "\n",
    "The configuration blocks fall into a few cateogies:\n",
    "\n",
    " - Net: the global configuration at the top of the configuration file. It declares the size of input images, batch size, learning rate, and so on.\n",
    " \n",
    "      ```python\n",
    "        batch=64\n",
    "        subdivisions=16\n",
    "        width=608\n",
    "        height=608\n",
    "        channels=3\n",
    "        momentum=0.9\n",
    "        decay=0.0005\n",
    "        angle=0\n",
    "        saturation = 1.5\n",
    "        exposure = 1.5\n",
    "        hue=.1\n",
    "        \n",
    "      ```\n",
    "\n",
    " - Convolutional: convolutional layer. Note that this specfication is a little more powerful than the PyTorch way of doing things, as options\n",
    "   for batch normalization and ReLU are built in.\n",
    " \n",
    "      ```python\n",
    "        [convolutional]\n",
    "        batch_normalize=1\n",
    "        filters=32\n",
    "        size=3\n",
    "        stride=1\n",
    "        pad=1\n",
    "        activation=leaky\n",
    "        \n",
    "      ```\n",
    "\n",
    " - Shortcut: skip connections that implement residual blocks. -3 means to add the feature maps output by the previous layer to those output by the layer three layers\n",
    "   back. As far as I can tell, linear actually means identity (no projection).\n",
    "  \n",
    "      ```python\n",
    "        [shortcut]\n",
    "        from=-3           # Connect the layer three layers back to here.\n",
    "        activation=linear\n",
    "        \n",
    "      ```\n",
    "      \n",
    " - Upsample: Bilinear upsampling of the previous layer using a particular stride\n",
    "\n",
    "      ```python\n",
    "        [upsample]\n",
    "        stride=2\n",
    "\n",
    "      ```\n",
    "      \n",
    " - Route: The route layer deserves a bit of explanation. It has an attribute `layers`, which can have either one or two values.\n",
    " \n",
    "      ```python\n",
    "          [route]\n",
    "          layers = -4\n",
    "\n",
    "          [route]\n",
    "          layers = -1, 61\n",
    "        \n",
    "      ```\n",
    "\n",
    "   When the layers attribute has only one value, it outputs the feature maps of the layer indexed by the value. In our example, it is -4, so the layer will output\n",
    "   the feature maps from the 4th layer backwards from the route layer.\n",
    "\n",
    "   When layers has two values, it returns the concatenated feature maps of the layers indexed by its values. In our example it is -1, 61, so the layer will output\n",
    "   feature maps from the previous layer (-1) and the 61st layer, concatenated along the channels (depth) dimension.\n",
    "   \n",
    " - Yolo:\n",
    " \n",
    "     ```python\n",
    "          [yolo]\n",
    "          mask = 0,1,2\n",
    "          anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "          classes=80\n",
    "          num=9\n",
    "          jitter=.3\n",
    "          ignore_thresh = .5\n",
    "          truth_thresh = 1\n",
    "          random=1\n",
    "            \n",
    "     ```\n",
    "   Here we have a few important attributes:\n",
    "     - anchors: describes the anchor boxes. The model contains 9 anchors, but only those in the `mask` are used.\n",
    "\n",
    "     - mask: which anchors index will be used in this yolo layer\n",
    "     \n",
    "     - classes: number of object classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a network from the config file\n",
    "\n",
    "Go to `darknet.py`, and look at the `parse_cfg` function. The function will read configuration file and store the blocks in a dictionary.\n",
    "\n",
    "<img src=\"img/configfunc.JPG\" title=\"configfunc\" style=\"width: 640px;\" />\n",
    "\n",
    "Then create building blocks by using `create_modules` function. Take a look it for more understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covolutional block\n",
    "\n",
    "<img src=\"img/convolutionalblock.JPG\" title=\"covolutionalblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shortcut block\n",
    "\n",
    "<img src=\"img/shortcutblock.JPG\" title=\"shortcutblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsample block\n",
    "\n",
    "<img src=\"img/upsampleblock.JPG\" title=\"upsampleblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### route block\n",
    "\n",
    "Empty layer, why? :)\n",
    "Fancy pytorch\n",
    "\n",
    "<img src=\"img/routeblock.JPG\" title=\"routeblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yolo block\n",
    "\n",
    "<img src=\"img/yoloblock.JPG\" title=\"yoloblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the code\n",
    "\n",
    "You may need to run\n",
    "\n",
    "    # apt install libgl1-mesa-glx\n",
    "\n",
    "for the next step to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'type': 'net', 'batch': '1', 'subdivisions': '1', 'width': '416', 'height': '416', 'channels': '3', 'momentum': '0.9', 'decay': '0.0005', 'angle': '0', 'saturation': '1.5', 'exposure': '1.5', 'hue': '.1', 'learning_rate': '0.001', 'burn_in': '1000', 'max_batches': '500200', 'policy': 'steps', 'steps': '400000,450000', 'scales': '.1,.1'}, ModuleList(\n",
      "  (0): Sequential(\n",
      "    (conv_0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_0): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (conv_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (conv_2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (conv_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (shortcut_4): EmptyLayer()\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (conv_5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch_norm_5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (conv_6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (conv_7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (8): Sequential(\n",
      "    (shortcut_8): EmptyLayer()\n",
      "  )\n",
      "  (9): Sequential(\n",
      "    (conv_9): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (10): Sequential(\n",
      "    (conv_10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (11): Sequential(\n",
      "    (shortcut_11): EmptyLayer()\n",
      "  )\n",
      "  (12): Sequential(\n",
      "    (conv_12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch_norm_12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (13): Sequential(\n",
      "    (conv_13): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (14): Sequential(\n",
      "    (conv_14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (15): Sequential(\n",
      "    (shortcut_15): EmptyLayer()\n",
      "  )\n",
      "  (16): Sequential(\n",
      "    (conv_16): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (17): Sequential(\n",
      "    (conv_17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_17): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (18): Sequential(\n",
      "    (shortcut_18): EmptyLayer()\n",
      "  )\n",
      "  (19): Sequential(\n",
      "    (conv_19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_19): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (20): Sequential(\n",
      "    (conv_20): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_20): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (21): Sequential(\n",
      "    (shortcut_21): EmptyLayer()\n",
      "  )\n",
      "  (22): Sequential(\n",
      "    (conv_22): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_22): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (23): Sequential(\n",
      "    (conv_23): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_23): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (24): Sequential(\n",
      "    (shortcut_24): EmptyLayer()\n",
      "  )\n",
      "  (25): Sequential(\n",
      "    (conv_25): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_25): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (26): Sequential(\n",
      "    (conv_26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_26): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (27): Sequential(\n",
      "    (shortcut_27): EmptyLayer()\n",
      "  )\n",
      "  (28): Sequential(\n",
      "    (conv_28): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_28): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (29): Sequential(\n",
      "    (conv_29): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_29): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (30): Sequential(\n",
      "    (shortcut_30): EmptyLayer()\n",
      "  )\n",
      "  (31): Sequential(\n",
      "    (conv_31): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_31): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (32): Sequential(\n",
      "    (conv_32): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_32): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (33): Sequential(\n",
      "    (shortcut_33): EmptyLayer()\n",
      "  )\n",
      "  (34): Sequential(\n",
      "    (conv_34): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_34): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (35): Sequential(\n",
      "    (conv_35): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_35): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (36): Sequential(\n",
      "    (shortcut_36): EmptyLayer()\n",
      "  )\n",
      "  (37): Sequential(\n",
      "    (conv_37): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch_norm_37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_37): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (38): Sequential(\n",
      "    (conv_38): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_38): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (39): Sequential(\n",
      "    (conv_39): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_39): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_39): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (40): Sequential(\n",
      "    (shortcut_40): EmptyLayer()\n",
      "  )\n",
      "  (41): Sequential(\n",
      "    (conv_41): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_41): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (42): Sequential(\n",
      "    (conv_42): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_42): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (43): Sequential(\n",
      "    (shortcut_43): EmptyLayer()\n",
      "  )\n",
      "  (44): Sequential(\n",
      "    (conv_44): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_44): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_44): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (45): Sequential(\n",
      "    (conv_45): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_45): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (46): Sequential(\n",
      "    (shortcut_46): EmptyLayer()\n",
      "  )\n",
      "  (47): Sequential(\n",
      "    (conv_47): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_47): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_47): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (48): Sequential(\n",
      "    (conv_48): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_48): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (49): Sequential(\n",
      "    (shortcut_49): EmptyLayer()\n",
      "  )\n",
      "  (50): Sequential(\n",
      "    (conv_50): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_50): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_50): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (51): Sequential(\n",
      "    (conv_51): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_51): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_51): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (52): Sequential(\n",
      "    (shortcut_52): EmptyLayer()\n",
      "  )\n",
      "  (53): Sequential(\n",
      "    (conv_53): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_53): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_53): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (54): Sequential(\n",
      "    (conv_54): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_54): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (55): Sequential(\n",
      "    (shortcut_55): EmptyLayer()\n",
      "  )\n",
      "  (56): Sequential(\n",
      "    (conv_56): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_56): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_56): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (57): Sequential(\n",
      "    (conv_57): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_57): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_57): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (58): Sequential(\n",
      "    (shortcut_58): EmptyLayer()\n",
      "  )\n",
      "  (59): Sequential(\n",
      "    (conv_59): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_59): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_59): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (60): Sequential(\n",
      "    (conv_60): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_60): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_60): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (61): Sequential(\n",
      "    (shortcut_61): EmptyLayer()\n",
      "  )\n",
      "  (62): Sequential(\n",
      "    (conv_62): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (batch_norm_62): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_62): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (63): Sequential(\n",
      "    (conv_63): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_63): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_63): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (64): Sequential(\n",
      "    (conv_64): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_64): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_64): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (65): Sequential(\n",
      "    (shortcut_65): EmptyLayer()\n",
      "  )\n",
      "  (66): Sequential(\n",
      "    (conv_66): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_66): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_66): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (67): Sequential(\n",
      "    (conv_67): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_67): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_67): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (68): Sequential(\n",
      "    (shortcut_68): EmptyLayer()\n",
      "  )\n",
      "  (69): Sequential(\n",
      "    (conv_69): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_69): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_69): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (70): Sequential(\n",
      "    (conv_70): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_70): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_70): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (71): Sequential(\n",
      "    (shortcut_71): EmptyLayer()\n",
      "  )\n",
      "  (72): Sequential(\n",
      "    (conv_72): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_72): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_72): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (73): Sequential(\n",
      "    (conv_73): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_73): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_73): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (74): Sequential(\n",
      "    (shortcut_74): EmptyLayer()\n",
      "  )\n",
      "  (75): Sequential(\n",
      "    (conv_75): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_75): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_75): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (76): Sequential(\n",
      "    (conv_76): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_76): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_76): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (77): Sequential(\n",
      "    (conv_77): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_77): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_77): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (78): Sequential(\n",
      "    (conv_78): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_78): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_78): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (79): Sequential(\n",
      "    (conv_79): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_79): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_79): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (80): Sequential(\n",
      "    (conv_80): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_80): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_80): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (81): Sequential(\n",
      "    (conv_81): Conv2d(1024, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (82): Sequential(\n",
      "    (Detection_82): DetectionLayer()\n",
      "  )\n",
      "  (83): Sequential(\n",
      "    (route_83): EmptyLayer()\n",
      "  )\n",
      "  (84): Sequential(\n",
      "    (conv_84): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_84): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_84): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (85): Sequential(\n",
      "    (upsample_85): Upsample(scale_factor=2.0, mode=nearest)\n",
      "  )\n",
      "  (86): Sequential(\n",
      "    (route_86): EmptyLayer()\n",
      "  )\n",
      "  (87): Sequential(\n",
      "    (conv_87): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_87): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_87): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (88): Sequential(\n",
      "    (conv_88): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_88): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_88): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (89): Sequential(\n",
      "    (conv_89): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_89): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_89): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (90): Sequential(\n",
      "    (conv_90): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_90): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_90): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (91): Sequential(\n",
      "    (conv_91): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_91): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_91): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (92): Sequential(\n",
      "    (conv_92): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_92): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_92): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (93): Sequential(\n",
      "    (conv_93): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (94): Sequential(\n",
      "    (Detection_94): DetectionLayer()\n",
      "  )\n",
      "  (95): Sequential(\n",
      "    (route_95): EmptyLayer()\n",
      "  )\n",
      "  (96): Sequential(\n",
      "    (conv_96): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_96): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_96): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (97): Sequential(\n",
      "    (upsample_97): Upsample(scale_factor=2.0, mode=nearest)\n",
      "  )\n",
      "  (98): Sequential(\n",
      "    (route_98): EmptyLayer()\n",
      "  )\n",
      "  (99): Sequential(\n",
      "    (conv_99): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_99): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_99): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (100): Sequential(\n",
      "    (conv_100): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_100): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_100): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (101): Sequential(\n",
      "    (conv_101): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_101): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_101): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (102): Sequential(\n",
      "    (conv_102): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_102): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_102): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (103): Sequential(\n",
      "    (conv_103): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (batch_norm_103): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_103): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (104): Sequential(\n",
      "    (conv_104): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (batch_norm_104): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_104): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (105): Sequential(\n",
      "    (conv_105): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (106): Sequential(\n",
      "    (Detection_106): DetectionLayer()\n",
      "  )\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "import darknet\n",
    "\n",
    "blocks = darknet.parse_cfg(\"cfg/yolov3.cfg\")\n",
    "print(darknet.create_modules(blocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet class\n",
    "\n",
    "The Darknet class is in darknet.py. However, we will see the class details\n",
    "\n",
    "The class has 2 main functions:\n",
    "\n",
    "1. Forward propagation: follow the instruction from dictionary modules\n",
    "2. Load weight: The function is used for load pretrained weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "class MyDarknet(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(MyDarknet, self).__init__()\n",
    "        # load the config file and create our model\n",
    "        self.blocks = darknet.parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = darknet.create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self, x, CUDA:bool):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        # run forward propagation. Follow the instruction from dictionary modules\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
    "                # do convolutional network\n",
    "                x = self.module_list[i](x)\n",
    "    \n",
    "            elif module_type == \"route\":\n",
    "                # concat layers\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "    \n",
    "                if (layers[0]) > 0:\n",
    "                    layers[0] = layers[0] - i\n",
    "    \n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[i + (layers[0])]\n",
    "    \n",
    "                else:\n",
    "                    if (layers[1]) > 0:\n",
    "                        layers[1] = layers[1] - i\n",
    "    \n",
    "                    map1 = outputs[i + layers[0]]\n",
    "                    map2 = outputs[i + layers[1]]\n",
    "                    x = torch.cat((map1, map2), 1)\n",
    "                \n",
    "    \n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                # residual network\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "    \n",
    "            elif module_type == 'yolo':        \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensions\n",
    "                inp_dim = int (self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int (module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                x = x.data\n",
    "                # predict_transform is in util.py\n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "        \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections\n",
    "\n",
    "\n",
    "    def load_weights(self, weightfile):\n",
    "        '''\n",
    "        Load pretrained weight\n",
    "        '''\n",
    "        #Open the weights file\n",
    "        fp = open(weightfile, \"rb\")\n",
    "    \n",
    "        #The first 5 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4,5. Images seen by the network (during training)\n",
    "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]   \n",
    "        \n",
    "        weights = np.fromfile(fp, dtype = np.float32)\n",
    "        \n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "    \n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            \n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "            \n",
    "                conv = model[0]\n",
    "                \n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "        \n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "        \n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "        \n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "        \n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    #Number of biases\n",
    "                    num_biases = conv.bias.numel()\n",
    "                \n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                \n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                \n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "                \n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-05 01:57:05--  https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png [following]\n",
      "--2021-02-05 01:57:06--  https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 347445 (339K) [image/png]\n",
      "Saving to: 'dog-cycle-car.png'\n",
      "\n",
      "dog-cycle-car.png   100%[===================>] 339.30K  2.21MB/s    in 0.2s    \n",
      "\n",
      "2021-02-05 01:57:06 (2.21 MB/s) - 'dog-cycle-car.png' saved [347445/347445]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (416,416))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5927e+01, 1.6094e+01, 1.6675e+02,  ..., 4.2245e-01,\n",
      "          4.6458e-01, 4.7548e-01],\n",
      "         [1.6830e+01, 1.4390e+01, 2.1259e+02,  ..., 5.2560e-01,\n",
      "          5.4941e-01, 5.7279e-01],\n",
      "         [1.4042e+01, 1.7765e+01, 3.9097e+02,  ..., 3.9175e-01,\n",
      "          4.1530e-01, 5.1767e-01],\n",
      "         ...,\n",
      "         [4.1260e+02, 4.1127e+02, 8.7575e+00,  ..., 5.8015e-01,\n",
      "          6.0855e-01, 3.7363e-01],\n",
      "         [4.1260e+02, 4.1191e+02, 1.2210e+01,  ..., 5.0164e-01,\n",
      "          5.1620e-01, 4.8353e-01],\n",
      "         [4.1152e+02, 4.1266e+02, 3.3448e+01,  ..., 4.5371e-01,\n",
      "          4.5869e-01, 4.4905e-01]]])\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "\n",
    "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
    "inp = get_test_input()\n",
    "pred = model(inp, False)\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output result\n",
    "\n",
    "The result from prediction model will be $B*10647*85$ which means\n",
    "\n",
    "- $B$: number of images in a batch\n",
    "- 10647: number of bounding boxes predicted for 1 images\n",
    "- 85: number of bounding box attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pretrained weight file\n",
    "\n",
    "Weights have been stored as in this diagram:\n",
    "\n",
    "<img src=\"img/weights.png\" title=\"weight\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-05 01:57:53--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: 'yolov3.weights'\n",
      "\n",
      "yolov3.weights      100%[===================>] 236.52M  4.80MB/s    in 63s     \n",
      "\n",
      "2021-02-05 01:58:57 (3.76 MB/s) - 'yolov3.weights' saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"yolov3.weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the sample image again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[8.5426e+00, 1.9015e+01, 1.1130e+02,  ..., 1.7306e-03,\n",
      "          1.3874e-03, 9.2985e-04],\n",
      "         [1.4105e+01, 1.8867e+01, 9.4014e+01,  ..., 5.9501e-04,\n",
      "          9.2471e-04, 1.3085e-03],\n",
      "         [2.1125e+01, 1.5269e+01, 3.5793e+02,  ..., 8.3609e-03,\n",
      "          5.1067e-03, 5.8562e-03],\n",
      "         ...,\n",
      "         [4.1268e+02, 4.1069e+02, 3.7157e+00,  ..., 1.7185e-06,\n",
      "          4.0955e-06, 6.5897e-07],\n",
      "         [4.1132e+02, 4.1023e+02, 8.0353e+00,  ..., 1.3927e-05,\n",
      "          3.2252e-05, 1.2076e-05],\n",
      "         [4.1076e+02, 4.1318e+02, 4.9635e+01,  ..., 4.2174e-06,\n",
      "          1.0794e-05, 1.8104e-05]]])\n"
     ]
    }
   ],
   "source": [
    "inp = get_test_input()\n",
    "pred = model(inp, False)\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a *true* detection?\n",
    "\n",
    "In the prediction result, there are too many results. Thus, it needs to threshold by using objectness score. We can use **write_results** function. It is in util.py.\n",
    "\n",
    "        def write_results(prediction, confidence, num_classes, nms_conf = 0.4)\n",
    "\n",
    "- prediction: prediction result\n",
    "- confidence: objectness score threshold\n",
    "- num_classes: number of classes\n",
    "- nms_conf: NMS IoU threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,  61.5403, 100.8597, 307.2717, 303.1132,   0.9469,   0.9985,\n",
       "           1.0000],\n",
       "        [  0.0000, 253.8484,  66.1096, 378.0396, 118.0089,   0.9992,   0.8164,\n",
       "           7.0000],\n",
       "        [  0.0000,  71.0338, 163.2243, 175.7471, 382.2702,   0.9999,   0.9936,\n",
       "          16.0000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_results(pred, 0.5, 80, nms_conf = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show image result in images\n",
    "\n",
    "The model was trained on the COCO dataset, so download the classes label file `coco.names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-05 02:00:02--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 625 [text/plain]\n",
      "Saving to: 'coco.names'\n",
      "\n",
      "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-05 02:00:03 (13.9 MB/s) - 'coco.names' saved [625/625]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
    "!mkdir data\n",
    "!mv coco.names data/coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 80\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our model by using the code below\n",
    "\n",
    "This code has been modified from **detect.py**. You can download it from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-05 02:00:12--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 7273 (7.1K) [text/plain]\n",
      "Saving to: 'detect.py'\n",
      "\n",
      "detect.py           100%[===================>]   7.10K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-05 02:00:13 (29.2 MB/s) - 'detect.py' saved [7273/7273]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network.....\n",
      "Network successfully loaded\n",
      "No file or directory with the name cocoimages\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'imlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-de02e1561cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mload_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mloaded_ims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mim_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_ims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_dim\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imlist' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "from darknet import Darknet\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "images = \"cocoimages\"\n",
    "batch_size = 4\n",
    "confidence = 0.5\n",
    "nms_thesh = 0.4\n",
    "start = 0\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "\n",
    "#Set up the neural network\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
    "model.load_weights(\"yolov3.weights\")\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = 416\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "#If there's a GPU availible, put the model on GPU\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "read_dir = time.time()\n",
    "\n",
    "# Detection phase\n",
    "\n",
    "try:\n",
    "    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
    "except NotADirectoryError:\n",
    "    imlist = []\n",
    "    imlist.append(osp.join(osp.realpath('.'), images))\n",
    "except FileNotFoundError:\n",
    "    print (\"No file or directory with the name {}\".format(images))\n",
    "    exit()\n",
    "    \n",
    "if not os.path.exists(\"des\"):\n",
    "    os.makedirs(\"des\")\n",
    "\n",
    "load_batch = time.time()\n",
    "loaded_ims = [cv2.imread(x) for x in imlist]\n",
    "\n",
    "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "\n",
    "\n",
    "leftover = 0\n",
    "if (len(im_dim_list) % batch_size):\n",
    "    leftover = 1\n",
    "\n",
    "if batch_size != 1:\n",
    "    num_batches = len(imlist) // batch_size + leftover            \n",
    "    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
    "                        len(im_batches))]))  for i in range(num_batches)]  \n",
    "\n",
    "write = 0\n",
    "\n",
    "if CUDA:\n",
    "    im_dim_list = im_dim_list.cuda()\n",
    "    \n",
    "start_det_loop = time.time()\n",
    "for i, batch in enumerate(im_batches):\n",
    "    # Load the image \n",
    "    start = time.time()\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(Variable(batch), CUDA)\n",
    "\n",
    "    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    if type(prediction) == int:\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            im_id = i*batch_size + im_num\n",
    "            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
    "            print(\"----------------------------------------------------------\")\n",
    "        continue\n",
    "\n",
    "    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
    "\n",
    "    if not write:                      #If we have't initialised output\n",
    "        output = prediction  \n",
    "        write = 1\n",
    "    else:\n",
    "        output = torch.cat((output,prediction))\n",
    "\n",
    "    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "        im_id = i*batch_size + im_num\n",
    "        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "        print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "        print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    if CUDA:\n",
    "        torch.cuda.synchronize()       \n",
    "try:\n",
    "    output\n",
    "except NameError:\n",
    "    print (\"No detections were made\")\n",
    "    exit()\n",
    "\n",
    "im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "\n",
    "scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1)\n",
    "\n",
    "output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "\n",
    "output[:,1:5] /= scaling_factor\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "    \n",
    "output_recast = time.time()\n",
    "class_load = time.time()\n",
    "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
    "\n",
    "draw = time.time()\n",
    "\n",
    "def write(x, results):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    img = results[int(x[0])]\n",
    "    cls = int(x[-1])\n",
    "    color = random.choice(colors)\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "\n",
    "list(map(lambda x: write(x, loaded_ims), output))\n",
    "\n",
    "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
    "\n",
    "list(map(cv2.imwrite, det_names, loaded_ims))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
    "print()\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hola!!! You got the YOLO result\n",
    "\n",
    "<img src=\"img/dogresult.png\" title=\"weight\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class homework: YOLO v4\n",
    "\n",
    "Create **yolov4.cfg** and modify **darknet.py** if necessary.\n",
    "\n",
    "Use COCO dataset for train or others.\n",
    "\n",
    "You can see more information in link: https://jonathan-hui.medium.com/yolov4-c9901eaa8e61\n",
    "\n",
    "The yolov4 network path way for input 416x416 is below:\n",
    "\n",
    "<img src=\"img/YoloV4.jpeg\" title=\"yolo4art\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download COCO dataset\n",
    "\n",
    "Link: https://medium.com/howtoai/pytorch-torchvision-coco-dataset-b7f5e8cad82\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'coco'...\n",
      "remote: Enumerating objects: 975, done.\u001b[K\n",
      "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
      "Receiving objects: 100% (975/975), 11.72 MiB | 5.35 MiB/s, done.\n",
      "Resolving deltas: 100% (576/576), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pdollar/coco/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      " extracting: val2017/000000155154.jpg  \n",
      " extracting: val2017/000000565012.jpg  \n",
      " extracting: val2017/000000284764.jpg  \n",
      " extracting: val2017/000000071711.jpg  \n",
      " extracting: val2017/000000565778.jpg  \n",
      " extracting: val2017/000000107851.jpg  \n",
      " extracting: val2017/000000516677.jpg  \n",
      " extracting: val2017/000000205105.jpg  \n",
      " extracting: val2017/000000144114.jpg  \n",
      " extracting: val2017/000000104612.jpg  \n",
      " extracting: val2017/000000037740.jpg  \n",
      " extracting: val2017/000000519338.jpg  \n",
      " extracting: val2017/000000240023.jpg  \n",
      " extracting: val2017/000000032941.jpg  \n",
      " extracting: val2017/000000500257.jpg  \n",
      " extracting: val2017/000000554838.jpg  \n",
      " extracting: val2017/000000292082.jpg  \n",
      " extracting: val2017/000000176634.jpg  \n",
      " extracting: val2017/000000212573.jpg  \n",
      " extracting: val2017/000000437898.jpg  \n",
      " extracting: val2017/000000328030.jpg  \n",
      " extracting: val2017/000000047010.jpg  \n",
      " extracting: val2017/000000199236.jpg  \n",
      " extracting: val2017/000000360137.jpg  \n",
      " extracting: val2017/000000157767.jpg  \n",
      " extracting: val2017/000000200667.jpg  \n",
      " extracting: val2017/000000053529.jpg  \n",
      " extracting: val2017/000000190676.jpg  \n",
      " extracting: val2017/000000154431.jpg  \n",
      " extracting: val2017/000000352582.jpg  \n",
      " extracting: val2017/000000367818.jpg  \n",
      " extracting: val2017/000000272212.jpg  \n",
      " extracting: val2017/000000531135.jpg  \n",
      " extracting: val2017/000000410487.jpg  \n",
      " extracting: val2017/000000090108.jpg  \n",
      " extracting: val2017/000000575500.jpg  \n",
      " extracting: val2017/000000082986.jpg  \n",
      " extracting: val2017/000000449661.jpg  \n",
      " extracting: val2017/000000384666.jpg  \n",
      " extracting: val2017/000000569700.jpg  \n",
      " extracting: val2017/000000264968.jpg  \n",
      " extracting: val2017/000000213593.jpg  \n",
      " extracting: val2017/000000293804.jpg  \n",
      " extracting: val2017/000000511076.jpg  \n",
      " extracting: val2017/000000061171.jpg  \n",
      " extracting: val2017/000000033707.jpg  \n",
      " extracting: val2017/000000424975.jpg  \n",
      " extracting: val2017/000000217219.jpg  \n",
      " extracting: val2017/000000427160.jpg  \n",
      " extracting: val2017/000000410934.jpg  \n",
      " extracting: val2017/000000214753.jpg  \n",
      " extracting: val2017/000000186422.jpg  \n",
      " extracting: val2017/000000034417.jpg  \n",
      " extracting: val2017/000000176232.jpg  \n",
      " extracting: val2017/000000176847.jpg  \n",
      " extracting: val2017/000000172547.jpg  \n",
      " extracting: val2017/000000423798.jpg  \n",
      " extracting: val2017/000000183437.jpg  \n",
      " extracting: val2017/000000501023.jpg  \n",
      " extracting: val2017/000000203639.jpg  \n",
      " extracting: val2017/000000376284.jpg  \n",
      " extracting: val2017/000000017905.jpg  \n",
      " extracting: val2017/000000078170.jpg  \n",
      " extracting: val2017/000000022969.jpg  \n",
      " extracting: val2017/000000270908.jpg  \n",
      " extracting: val2017/000000520531.jpg  \n",
      " extracting: val2017/000000114049.jpg  \n",
      " extracting: val2017/000000532058.jpg  \n",
      " extracting: val2017/000000427034.jpg  \n",
      " extracting: val2017/000000067534.jpg  \n",
      " extracting: val2017/000000184338.jpg  \n",
      " extracting: val2017/000000358525.jpg  \n",
      " extracting: val2017/000000365095.jpg  \n",
      " extracting: val2017/000000342397.jpg  \n",
      " extracting: val2017/000000168619.jpg  \n",
      " extracting: val2017/000000225184.jpg  \n",
      " extracting: val2017/000000094336.jpg  \n",
      " extracting: val2017/000000063047.jpg  \n",
      " extracting: val2017/000000133244.jpg  \n",
      " extracting: val2017/000000096549.jpg  \n",
      " extracting: val2017/000000266981.jpg  \n",
      " extracting: val2017/000000162035.jpg  \n",
      " extracting: val2017/000000408830.jpg  \n",
      " extracting: val2017/000000164637.jpg  \n",
      " extracting: val2017/000000215114.jpg  \n",
      " extracting: val2017/000000403122.jpg  \n",
      " extracting: val2017/000000580418.jpg  \n",
      " extracting: val2017/000000170739.jpg  \n",
      " extracting: val2017/000000451084.jpg  \n",
      " extracting: val2017/000000119828.jpg  \n",
      " extracting: val2017/000000223182.jpg  \n",
      " extracting: val2017/000000531495.jpg  \n",
      " extracting: val2017/000000317999.jpg  \n",
      " extracting: val2017/000000568690.jpg  \n",
      " extracting: val2017/000000516871.jpg  \n",
      " extracting: val2017/000000184324.jpg  \n",
      " extracting: val2017/000000228436.jpg  \n",
      " extracting: val2017/000000162581.jpg  \n",
      " extracting: val2017/000000426329.jpg  \n",
      " extracting: val2017/000000420840.jpg  \n",
      " extracting: val2017/000000473015.jpg  \n",
      " extracting: val2017/000000239627.jpg  \n",
      " extracting: val2017/000000541952.jpg  \n",
      " extracting: val2017/000000223747.jpg  \n",
      " extracting: val2017/000000481413.jpg  \n",
      " extracting: val2017/000000539445.jpg  \n",
      " extracting: val2017/000000357941.jpg  \n",
      " extracting: val2017/000000410456.jpg  \n",
      " extracting: val2017/000000222299.jpg  \n",
      " extracting: val2017/000000489924.jpg  \n",
      " extracting: val2017/000000058029.jpg  \n",
      " extracting: val2017/000000243075.jpg  \n",
      " extracting: val2017/000000137294.jpg  \n",
      " extracting: val2017/000000569059.jpg  \n",
      " extracting: val2017/000000370375.jpg  \n",
      " extracting: val2017/000000099810.jpg  \n",
      " extracting: val2017/000000122672.jpg  \n",
      " extracting: val2017/000000186449.jpg  \n",
      " extracting: val2017/000000445792.jpg  \n",
      " extracting: val2017/000000375493.jpg  \n",
      " extracting: val2017/000000183127.jpg  \n",
      " extracting: val2017/000000380711.jpg  \n",
      " extracting: val2017/000000442836.jpg  \n",
      " extracting: val2017/000000491071.jpg  \n",
      " extracting: val2017/000000026564.jpg  \n",
      " extracting: val2017/000000367082.jpg  \n",
      " extracting: val2017/000000464144.jpg  \n",
      " extracting: val2017/000000535306.jpg  \n",
      " extracting: val2017/000000463037.jpg  \n",
      " extracting: val2017/000000409198.jpg  \n",
      " extracting: val2017/000000445846.jpg  \n",
      " extracting: val2017/000000257865.jpg  \n",
      " extracting: val2017/000000166509.jpg  \n",
      " extracting: val2017/000000056344.jpg  \n",
      " extracting: val2017/000000069795.jpg  \n",
      " extracting: val2017/000000250619.jpg  \n",
      " extracting: val2017/000000173183.jpg  \n",
      " extracting: val2017/000000533855.jpg  \n",
      " extracting: val2017/000000364297.jpg  \n",
      " extracting: val2017/000000451571.jpg  \n",
      " extracting: val2017/000000025096.jpg  \n",
      " extracting: val2017/000000422836.jpg  \n",
      " extracting: val2017/000000078404.jpg  \n",
      " extracting: val2017/000000043816.jpg  \n",
      " extracting: val2017/000000528862.jpg  \n",
      " extracting: val2017/000000088462.jpg  \n",
      " extracting: val2017/000000253695.jpg  \n",
      " extracting: val2017/000000147729.jpg  \n",
      " extracting: val2017/000000079014.jpg  \n",
      " extracting: val2017/000000202001.jpg  \n",
      " extracting: val2017/000000244019.jpg  \n",
      " extracting: val2017/000000544306.jpg  \n",
      " extracting: val2017/000000259382.jpg  \n",
      " extracting: val2017/000000304365.jpg  \n",
      " extracting: val2017/000000301421.jpg  \n",
      " extracting: val2017/000000020571.jpg  \n",
      " extracting: val2017/000000157601.jpg  \n",
      " extracting: val2017/000000468505.jpg  \n",
      " extracting: val2017/000000088265.jpg  \n",
      " extracting: val2017/000000027696.jpg  \n",
      " extracting: val2017/000000234807.jpg  \n",
      " extracting: val2017/000000547383.jpg  \n",
      " extracting: val2017/000000499775.jpg  \n",
      " extracting: val2017/000000158660.jpg  \n",
      " extracting: val2017/000000173008.jpg  \n",
      " extracting: val2017/000000216516.jpg  \n",
      " extracting: val2017/000000071877.jpg  \n",
      " extracting: val2017/000000153669.jpg  \n",
      " extracting: val2017/000000520009.jpg  \n",
      " extracting: val2017/000000179112.jpg  \n",
      " extracting: val2017/000000378099.jpg  \n",
      " extracting: val2017/000000562197.jpg  \n",
      " extracting: val2017/000000130586.jpg  \n",
      " extracting: val2017/000000329456.jpg  \n",
      " extracting: val2017/000000314541.jpg  \n",
      " extracting: val2017/000000286907.jpg  \n",
      " extracting: val2017/000000000632.jpg  \n",
      " extracting: val2017/000000460147.jpg  \n",
      " extracting: val2017/000000249129.jpg  \n",
      " extracting: val2017/000000379800.jpg  \n",
      " extracting: val2017/000000029640.jpg  \n",
      " extracting: val2017/000000150638.jpg  \n",
      " extracting: val2017/000000480985.jpg  \n",
      " extracting: val2017/000000389532.jpg  \n",
      " extracting: val2017/000000351362.jpg  \n",
      " extracting: val2017/000000015338.jpg  \n",
      " extracting: val2017/000000492110.jpg  \n",
      " extracting: val2017/000000361103.jpg  \n",
      " extracting: val2017/000000375015.jpg  \n",
      " extracting: val2017/000000062025.jpg  \n",
      " extracting: val2017/000000370999.jpg  \n",
      " extracting: val2017/000000004134.jpg  \n",
      " extracting: val2017/000000057725.jpg  \n",
      " extracting: val2017/000000441286.jpg  \n",
      " extracting: val2017/000000377486.jpg  \n",
      " extracting: val2017/000000016451.jpg  \n",
      " extracting: val2017/000000347456.jpg  \n",
      " extracting: val2017/000000367195.jpg  \n",
      " extracting: val2017/000000269196.jpg  \n",
      " extracting: val2017/000000011699.jpg  \n",
      " extracting: val2017/000000309495.jpg  \n",
      " extracting: val2017/000000011813.jpg  \n",
      " extracting: val2017/000000237071.jpg  \n",
      " extracting: val2017/000000272566.jpg  \n",
      " extracting: val2017/000000132796.jpg  \n",
      " extracting: val2017/000000384949.jpg  \n",
      " extracting: val2017/000000276055.jpg  \n",
      " extracting: val2017/000000236721.jpg  \n",
      " extracting: val2017/000000286523.jpg  \n",
      " extracting: val2017/000000024027.jpg  \n",
      " extracting: val2017/000000462614.jpg  \n",
      " extracting: val2017/000000345261.jpg  \n",
      " extracting: val2017/000000295316.jpg  \n",
      " extracting: val2017/000000190637.jpg  \n",
      " extracting: val2017/000000172617.jpg  \n",
      " extracting: val2017/000000093717.jpg  \n",
      " extracting: val2017/000000425702.jpg  \n",
      " extracting: val2017/000000522889.jpg  \n",
      " extracting: val2017/000000160556.jpg  \n",
      " extracting: val2017/000000553511.jpg  \n",
      " extracting: val2017/000000170099.jpg  \n",
      " extracting: val2017/000000173799.jpg  \n",
      " extracting: val2017/000000488736.jpg  \n",
      " extracting: val2017/000000301135.jpg  \n",
      " extracting: val2017/000000018491.jpg  \n",
      " extracting: val2017/000000124277.jpg  \n",
      " extracting: val2017/000000488673.jpg  \n",
      " extracting: val2017/000000533816.jpg  \n",
      " extracting: val2017/000000172935.jpg  \n",
      " extracting: val2017/000000137576.jpg  \n",
      " extracting: val2017/000000520264.jpg  \n",
      " extracting: val2017/000000410650.jpg  \n",
      " extracting: val2017/000000117914.jpg  \n",
      " extracting: val2017/000000338901.jpg  \n",
      " extracting: val2017/000000223955.jpg  \n",
      " extracting: val2017/000000030675.jpg  \n",
      " extracting: val2017/000000530061.jpg  \n",
      " extracting: val2017/000000335954.jpg  \n",
      " extracting: val2017/000000428218.jpg  \n",
      " extracting: val2017/000000192670.jpg  \n",
      " extracting: val2017/000000447465.jpg  \n",
      " extracting: val2017/000000144984.jpg  \n",
      " extracting: val2017/000000212559.jpg  \n",
      " extracting: val2017/000000466339.jpg  \n",
      " extracting: val2017/000000015335.jpg  \n",
      " extracting: val2017/000000156924.jpg  \n",
      " extracting: val2017/000000211825.jpg  \n",
      " extracting: val2017/000000162732.jpg  \n",
      " extracting: val2017/000000118367.jpg  \n",
      " extracting: val2017/000000435208.jpg  \n",
      " extracting: val2017/000000341828.jpg  \n",
      " extracting: val2017/000000475365.jpg  \n",
      " extracting: val2017/000000493613.jpg  \n",
      " extracting: val2017/000000562581.jpg  \n",
      " extracting: val2017/000000047585.jpg  \n",
      " extracting: val2017/000000261535.jpg  \n",
      " extracting: val2017/000000306139.jpg  \n",
      " extracting: val2017/000000011051.jpg  \n",
      " extracting: val2017/000000086755.jpg  \n",
      " extracting: val2017/000000205289.jpg  \n",
      " extracting: val2017/000000149375.jpg  \n",
      " extracting: val2017/000000193245.jpg  \n",
      " extracting: val2017/000000216277.jpg  \n",
      " extracting: val2017/000000035197.jpg  \n",
      " extracting: val2017/000000048504.jpg  \n",
      " extracting: val2017/000000429011.jpg  \n",
      " extracting: val2017/000000217957.jpg  \n",
      " extracting: val2017/000000322895.jpg  \n",
      " extracting: val2017/000000015079.jpg  \n",
      " extracting: val2017/000000431140.jpg  \n",
      " extracting: val2017/000000169356.jpg  \n",
      " extracting: val2017/000000408696.jpg  \n",
      " extracting: val2017/000000338325.jpg  \n",
      " extracting: val2017/000000250137.jpg  \n",
      " extracting: val2017/000000454404.jpg  \n",
      " extracting: val2017/000000421060.jpg  \n",
      " extracting: val2017/000000073326.jpg  \n",
      " extracting: val2017/000000410878.jpg  \n",
      " extracting: val2017/000000292908.jpg  \n",
      " extracting: val2017/000000350679.jpg  \n",
      " extracting: val2017/000000390301.jpg  \n",
      " extracting: val2017/000000213547.jpg  \n",
      " extracting: val2017/000000087244.jpg  \n",
      " extracting: val2017/000000253819.jpg  \n",
      " extracting: val2017/000000192699.jpg  \n",
      " extracting: val2017/000000260261.jpg  \n",
      " extracting: val2017/000000044279.jpg  \n",
      " extracting: val2017/000000306136.jpg  \n",
      " extracting: val2017/000000066771.jpg  \n",
      " extracting: val2017/000000355257.jpg  \n",
      " extracting: val2017/000000548339.jpg  \n",
      " extracting: val2017/000000125062.jpg  \n",
      " extracting: val2017/000000078565.jpg  \n",
      " extracting: val2017/000000332845.jpg  \n",
      " extracting: val2017/000000298904.jpg  \n",
      " extracting: val2017/000000437351.jpg  \n",
      " extracting: val2017/000000232646.jpg  \n",
      " extracting: val2017/000000153217.jpg  \n",
      " extracting: val2017/000000377946.jpg  \n",
      " extracting: val2017/000000478136.jpg  \n",
      " extracting: val2017/000000458992.jpg  \n",
      " extracting: val2017/000000495448.jpg  \n",
      " extracting: val2017/000000221708.jpg  \n",
      " extracting: val2017/000000152214.jpg  \n",
      " extracting: val2017/000000493019.jpg  \n",
      " extracting: val2017/000000459195.jpg  \n",
      " extracting: val2017/000000135890.jpg  \n",
      " extracting: val2017/000000012062.jpg  \n",
      " extracting: val2017/000000349860.jpg  \n",
      " extracting: val2017/000000246436.jpg  \n",
      " extracting: val2017/000000474854.jpg  \n",
      " extracting: val2017/000000388903.jpg  \n",
      " extracting: val2017/000000156643.jpg  \n",
      " extracting: val2017/000000030828.jpg  \n",
      " extracting: val2017/000000318138.jpg  \n",
      " extracting: val2017/000000368456.jpg  \n",
      " extracting: val2017/000000156292.jpg  \n",
      " extracting: val2017/000000355905.jpg  \n",
      " extracting: val2017/000000016598.jpg  \n",
      " extracting: val2017/000000125472.jpg  \n",
      " extracting: val2017/000000037670.jpg  \n",
      " extracting: val2017/000000178744.jpg  \n",
      " extracting: val2017/000000382009.jpg  \n",
      " extracting: val2017/000000276024.jpg  \n",
      " extracting: val2017/000000345027.jpg  \n",
      " extracting: val2017/000000377113.jpg  \n",
      " extracting: val2017/000000140556.jpg  \n",
      " extracting: val2017/000000000139.jpg  \n",
      " extracting: val2017/000000525155.jpg  \n",
      " extracting: val2017/000000217753.jpg  \n",
      " extracting: val2017/000000215259.jpg  \n",
      " extracting: val2017/000000119365.jpg  \n",
      " extracting: val2017/000000276707.jpg  \n",
      " extracting: val2017/000000072852.jpg  \n",
      " extracting: val2017/000000377814.jpg  \n",
      " extracting: val2017/000000222118.jpg  \n",
      " extracting: val2017/000000404922.jpg  \n",
      " extracting: val2017/000000296649.jpg  \n",
      " extracting: val2017/000000161032.jpg  \n",
      " extracting: val2017/000000005529.jpg  \n",
      " extracting: val2017/000000322864.jpg  \n",
      " extracting: val2017/000000194716.jpg  \n",
      " extracting: val2017/000000175364.jpg  \n",
      " extracting: val2017/000000001268.jpg  \n",
      " extracting: val2017/000000018193.jpg  \n",
      " extracting: val2017/000000515266.jpg  \n",
      " extracting: val2017/000000335081.jpg  \n",
      " extracting: val2017/000000094614.jpg  \n",
      " extracting: val2017/000000128748.jpg  \n",
      " extracting: val2017/000000568439.jpg  \n",
      " extracting: val2017/000000104198.jpg  \n",
      " extracting: val2017/000000003501.jpg  \n",
      " extracting: val2017/000000138492.jpg  \n",
      " extracting: val2017/000000228942.jpg  \n",
      " extracting: val2017/000000516143.jpg  \n",
      " extracting: val2017/000000179214.jpg  \n",
      " extracting: val2017/000000531707.jpg  \n",
      " extracting: val2017/000000298994.jpg  \n",
      " extracting: val2017/000000020107.jpg  \n",
      " extracting: val2017/000000241677.jpg  \n",
      " extracting: val2017/000000284725.jpg  \n",
      " extracting: val2017/000000117908.jpg  \n",
      " extracting: val2017/000000226058.jpg  \n",
      " extracting: val2017/000000506454.jpg  \n",
      " extracting: val2017/000000042888.jpg  \n",
      " extracting: val2017/000000167486.jpg  \n",
      " extracting: val2017/000000279927.jpg  \n",
      " extracting: val2017/000000383289.jpg  \n",
      " extracting: val2017/000000190236.jpg  \n",
      " extracting: val2017/000000375078.jpg  \n",
      " extracting: val2017/000000472030.jpg  \n",
      " extracting: val2017/000000447789.jpg  \n",
      " extracting: val2017/000000496409.jpg  \n",
      " extracting: val2017/000000263969.jpg  \n",
      " extracting: val2017/000000293474.jpg  \n",
      " extracting: val2017/000000025386.jpg  \n",
      " extracting: val2017/000000112634.jpg  \n",
      " extracting: val2017/000000186624.jpg  \n",
      " extracting: val2017/000000515077.jpg  \n",
      " extracting: val2017/000000523194.jpg  \n",
      " extracting: val2017/000000204871.jpg  \n",
      " extracting: val2017/000000257084.jpg  \n",
      " extracting: val2017/000000311392.jpg  \n",
      " extracting: val2017/000000191761.jpg  \n",
      " extracting: val2017/000000394328.jpg  \n",
      " extracting: val2017/000000162092.jpg  \n",
      " extracting: val2017/000000355325.jpg  \n",
      " extracting: val2017/000000027620.jpg  \n",
      " extracting: val2017/000000378453.jpg  \n",
      " extracting: val2017/000000330554.jpg  \n",
      " extracting: val2017/000000372819.jpg  \n",
      " extracting: val2017/000000526706.jpg  \n",
      " extracting: val2017/000000338986.jpg  \n",
      " extracting: val2017/000000561009.jpg  \n",
      " extracting: val2017/000000571008.jpg  \n",
      " extracting: val2017/000000154705.jpg  \n",
      " extracting: val2017/000000328286.jpg  \n",
      " extracting: val2017/000000116208.jpg  \n",
      " extracting: val2017/000000263860.jpg  \n",
      " extracting: val2017/000000229221.jpg  \n",
      " extracting: val2017/000000007108.jpg  \n",
      " extracting: val2017/000000252507.jpg  \n",
      " extracting: val2017/000000281759.jpg  \n",
      " extracting: val2017/000000523100.jpg  \n",
      " extracting: val2017/000000165713.jpg  \n",
      " extracting: val2017/000000242724.jpg  \n",
      " extracting: val2017/000000234779.jpg  \n",
      " extracting: val2017/000000465675.jpg  \n",
      " extracting: val2017/000000504000.jpg  \n",
      " extracting: val2017/000000298251.jpg  \n",
      " extracting: val2017/000000015660.jpg  \n",
      " extracting: val2017/000000111086.jpg  \n",
      " extracting: val2017/000000279730.jpg  \n",
      " extracting: val2017/000000331075.jpg  \n",
      " extracting: val2017/000000336587.jpg  \n",
      " extracting: val2017/000000350002.jpg  \n",
      " extracting: val2017/000000390555.jpg  \n",
      " extracting: val2017/000000018380.jpg  \n",
      " extracting: val2017/000000114907.jpg  \n",
      " extracting: val2017/000000357978.jpg  \n",
      " extracting: val2017/000000133819.jpg  \n",
      " extracting: val2017/000000487583.jpg  \n",
      " extracting: val2017/000000278705.jpg  \n",
      " extracting: val2017/000000380706.jpg  \n",
      " extracting: val2017/000000229111.jpg  \n",
      " extracting: val2017/000000283038.jpg  \n",
      " extracting: val2017/000000365655.jpg  \n",
      " extracting: val2017/000000049269.jpg  \n",
      " extracting: val2017/000000402118.jpg  \n",
      " extracting: val2017/000000239537.jpg  \n",
      " extracting: val2017/000000298738.jpg  \n",
      " extracting: val2017/000000334371.jpg  \n",
      " extracting: val2017/000000263403.jpg  \n",
      " extracting: val2017/000000106563.jpg  \n",
      " extracting: val2017/000000449603.jpg  \n",
      " extracting: val2017/000000346232.jpg  \n",
      " extracting: val2017/000000248284.jpg  \n",
      " extracting: val2017/000000572620.jpg  \n",
      " extracting: val2017/000000395701.jpg  \n",
      " extracting: val2017/000000054164.jpg  \n",
      " extracting: val2017/000000171382.jpg  \n",
      " extracting: val2017/000000513181.jpg  \n",
      " extracting: val2017/000000161781.jpg  \n",
      " extracting: val2017/000000394199.jpg  \n",
      " extracting: val2017/000000301718.jpg  \n",
      " extracting: val2017/000000555050.jpg  \n",
      " extracting: val2017/000000388846.jpg  \n",
      " extracting: val2017/000000323895.jpg  \n",
      " extracting: val2017/000000234660.jpg  \n",
      " extracting: val2017/000000343453.jpg  \n",
      " extracting: val2017/000000540928.jpg  \n",
      " extracting: val2017/000000190756.jpg  \n",
      " extracting: val2017/000000068387.jpg  \n",
      " extracting: val2017/000000151000.jpg  \n",
      " extracting: val2017/000000244592.jpg  \n",
      " extracting: val2017/000000158956.jpg  \n",
      " extracting: val2017/000000058636.jpg  \n",
      " extracting: val2017/000000326174.jpg  \n",
      " extracting: val2017/000000241319.jpg  \n",
      " extracting: val2017/000000244379.jpg  \n",
      " extracting: val2017/000000263796.jpg  \n",
      " extracting: val2017/000000176799.jpg  \n",
      " extracting: val2017/000000491867.jpg  \n",
      " extracting: val2017/000000017899.jpg  \n",
      " extracting: val2017/000000455716.jpg  \n",
      " extracting: val2017/000000284991.jpg  \n",
      " extracting: val2017/000000084431.jpg  \n",
      " extracting: val2017/000000284762.jpg  \n",
      " extracting: val2017/000000255536.jpg  \n",
      " extracting: val2017/000000043435.jpg  \n",
      " extracting: val2017/000000546325.jpg  \n",
      " extracting: val2017/000000291619.jpg  \n",
      " extracting: val2017/000000512648.jpg  \n",
      " extracting: val2017/000000014226.jpg  \n",
      " extracting: val2017/000000084492.jpg  \n",
      " extracting: val2017/000000376478.jpg  \n",
      " extracting: val2017/000000124636.jpg  \n",
      " extracting: val2017/000000564091.jpg  \n",
      " extracting: val2017/000000477689.jpg  \n",
      " extracting: val2017/000000523957.jpg  \n",
      " extracting: val2017/000000570539.jpg  \n",
      " extracting: val2017/000000470121.jpg  \n",
      " extracting: val2017/000000199442.jpg  \n",
      " extracting: val2017/000000563653.jpg  \n",
      " extracting: val2017/000000181421.jpg  \n",
      " extracting: val2017/000000302990.jpg  \n",
      " extracting: val2017/000000446005.jpg  \n",
      " extracting: val2017/000000329219.jpg  \n",
      " extracting: val2017/000000388258.jpg  \n",
      " extracting: val2017/000000126137.jpg  \n",
      " extracting: val2017/000000500826.jpg  \n",
      "Archive:  annotations_trainval2017.zip\n",
      "  inflating: annotations/instances_train2017.json  \n",
      "  inflating: annotations/instances_val2017.json  \n",
      "  inflating: annotations/captions_train2017.json  \n",
      "  inflating: annotations/captions_val2017.json  \n",
      "  inflating: annotations/person_keypoints_train2017.json  \n",
      "  inflating: annotations/person_keypoints_val2017.json  \n"
     ]
    }
   ],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\n",
    "!unzip train2017.zip\n",
    "!unzip val2017.zip\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "\n",
    "path2data=\"./train2017\"\n",
    "path2json=\"./annotations/instances_train2017.json\"\n",
    "\n",
    "coco_train = dset.CocoDetection(root = path2data, annFile = path2json)\n",
    "\n",
    "print('Number of samples: ', len(coco_train))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
